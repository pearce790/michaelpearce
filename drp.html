<!doctype html>
<html>

<head>
  <title>Michael Pearce</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">
        
        <!--Start Text Only-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">UW Statistics Directed Reading Program</h2>
            <hr>
            <p class="text">
              In addition to my more formal experiences as a teaching assistant, I've led numerous projects with undergraduate students as part of the UW
              Statistics Directed Reading Program (DRP). The one-on-one experiences are intended to help undergraduates from a wide variety of backgrounds
              gain exposure to areas of statistics not often covered in the curriculum and learn about life as a graduate student and researcher. For more 
              details of the program, click <a href="https://spa-drp.github.io/">here</a>. Below are the titles, prequisites, descriptions, and syllabi 
              of my DRP offerings. 
            </p>
            
            <h3>Voting, Ranking, and Preference Modeling (Autumn 2021)</h3>
            <p class="text">
              <i> Student:</i> Carolina Sawyer
            </p>
            <p class="text">
              <i> Prerequisites:</i> Stat 311 or equivalent.
            </p>
            <p class="text">
              <i> Description:</i> Preference data appears in many forms: voters deciding between candidates in an election, movie critics rating 
              new releases, and search engines ranking web pages, to name a few! However, modeling preferences in a statistical manner can be challenging 
              for a variety of reasons, such as computational difficulties in working with discrete and high-dimensional data. In this project, we will 
              study a variety of models used for preference data, which includes both ranking and scoring models. Understanding challenges and uncertainty 
              in aggregating preferences will be a key focus. Together, we will also carry out an applied project on preference data based on the 
              student's interests.
            </p>
            <p class="text">
              <i> Syllabus:</i>
            </p>
            <ul>
              <li>Week 1: Introduction to Preferences, Rankings, Ratings, and Voting. As you read these, don't try to understand every technical detail 
                or memorize the information (there's no quiz at the end). Instead, focus on the big picture - how do each of these articles, in various 
                levels of technicality, relate to understanding the preferences of people or systems? How do different methods of collecting, aggregating,
                or displaying preferences change behaviors or outcomes? What is confusing? What did you already know?</li>
                <ul>
                  <li><a href="https://www.fairvote.org/types_of_voting_systems">"Type of Voting Systems"</a> from FairVote (read the 3 articles linked on this page too)</li>
                  <li><a href="https://collider.com/how-best-picture-oscar-voting-works/">"Here's How the Complicated Best Picture Voting System Works"</a> from Collider</li>
                  <li>de Gemmis, et al., <a href="https://link.springer.com/chapter/10.1007%2F978-3-642-14125-6_18">"Learning Preference Models in Recommender Systems"</a> </li>
                  <li><a href="https://towardsdatascience.com/deep-dive-into-netflixs-recommender-system-341806ae3b48">"Deep Dive into Netflix's Recommender System"</a> on TowardsDataScience</li>
                  <li><a href="https://www.google.com/search/howsearchworks/algorithms/">"How Search algorithms work"</a> </li>
                </ul>
              <li>Week 2: Voting Systems</li>
                <ul>
                  <li><a href="https://www.npr.org/2021/06/30/1011747612/the-human-error-thats-snarling-the-new-york-city-mayors-race">"The 'Human Error' That's Snarling The New York City Mayor's Race"</a> by Joe Hernandez for NPR (2021)</li>
                  <li><a href="https://en.wikipedia.org/wiki/Single_transferable_vote">Single Transferable Vote</a> (focus on understanding definition, pros/cons, and challenges of determining the winner)</li>
                  <li><a href="http://tech.mit.edu/V123/N8/8voting.8n.html">"Arrow’s Theorem Proves No Voting System is Perfect"</a> by Nathan Collins for The Tech (2003)</li>
                  <li><a href="https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/">"How FiveThirtyEight’s 2020 Presidential Forecast Works — And What’s Different Because Of COVID-19"</a> by Nate Silver (2020)</li>
                  <li>Exercise: Imagine there are 100 voters in an election with 5 candidates (A, B, C, D, E) in which votes will be counted using ranked choice voting. Each voter provides a complete ranking of the candidates (e.g., one such 'vote' could be <E,C,B,A,D>). Write out an algorithm for counting
                    votes and determining a winner in the election. No need to implement this algorithm using code, but you may if you wish!
              </ul>
              <li>Week 3: Ranking Models I. Bradley-Terry and Plackett-Luce models; Luce's Choice Axiom. In each of these papers, focus mostly on the model formulation/applications, and less on the theory/estimation.</li>
                <ul>
                  <li>Bradley-Terry model: Chapter 11.6-11.6.3 in "Categorical Data Analysis" by Agresti (2013), accesible online at <a href="https://alliance-primo.hosted.exlibrisgroup.com/permalink/f/kjtuig/CP51230829810001451">UW Libraries</a>.</li>
                  <li>Plackett-Luce model: <a href="https://www.jstor.org/stable/2346567">"The Analysis of Permutations"</a>, Plackett (1975)</li>
                  <li>Luce's Choice Axiom: An introduction paper by <a href="https://escholarship.org/uc/item/02b7s10w">Yellott (2001)</a></li>
                  <li>Question and Exercise: How are the BT and PL models similar to or different from one another? Come up with a few examples for situations in which each could be used.</li>
                </ul>
              <li>Week 4: Ranking Models II. Rank distances, Mallows' models. This week, you'll do some independent research, read an important ranking paper, and try out some code in R for ranking models.</li>
                <ul>
                  <li>Rank distances: Learn about the following six distance metrics between rankings: Spearman's Footrule, Spearman's Rank Correlation, Hamming distance, Kendall's tau, Cayley distance, and Ulam's distance. A 
                  good place to start learning about them is <a href="https://www.jstor.org/stable/pdf/4355560.pdf">here (pages 112-119)</a>, but feel free to look for other resources as well. Once you understand them, write down two rankings of at least 6 objects and calculate the distance
                  between them using each of the aforementioned metrics. How similar or different are they? Can you write down two rankings that are close on some metrics but far apart on others?</li>
                  <li>Mallows' model: Read this seminal paper on the Mallows' model:  <a href="https://www.jstor.org/stable/pdf/2345433.pdf">"Distance Based Ranking Models"</a> by Fligner and Verducci (1986).</li>
                  <li>Exercise: Install and load the "PerMallows" package in R. Then, generate samples from a Mallows' distribution with a central ranking and scale parameter of your choosing (use the function "rmm"). After, fit a Mallows' model to the
                  data you generated (use the function "lmm"). Repeat this many times, and see how often the estimated central ranking is the central ranking you provided initially. How accurate was estimation of the scale parameter, theta? Is there a connection between the accuracy of the two?</li>
                  </ul>
              <li>Week 5: Coding Preference Learning Models. This week, you'll write code to calculate distances between rankings and to find consensus rankings. For each exercise, be sure to demonstrate your functions with examples or illustrations!</li>
                <ul>
                  <li>Exercise 1: Write a function in R to calculate the distance between two rankings. The function should include an argument to specify which distance metric to use, with options being Spearman's Footrule, Spearman's Rank Correlation, Hamming, Kendall's tau, and Cayley.</li>
                  <li>Exercise 2: Write a function in R to calculate the total distance between a "central ranking" and a collection of rankings (i.e., the sum of the distances from the central ranking to each ranking in the collection). Again, your function should allow the user to specify a distance metric.</li>
                  <li>Exercise 3: Write a function that brute-force calculates the "optimal" consensus ranking given a collection of rankings and a distance metric. Optimal means the ranking that has the smallest total distance to the rankings in the collection. If multiple rankings have the smallest total distance, output them all. This function should be used only when the number of objects is 6 or fewer.</li>
                  <li>Exercise 4: Write a function that estimates the "optimal" consensus ranking based on the Kendall's tau distance metric (use the "average rank" procedure discussed last week). This function should be used when the number of objects is greater than 6.</li>
                  <li>Exercise 5: Run a simulation in which you compare the accuracy of the brute-force and estimation procedures to the true consensus ranking, in which you generate rankings using a Mallows distribution for 5 objects, and varying scale parameters. Visualize your results!</li>
                </ul>
              <li>Week 6: Philosophy of Consensus, Preference Groups, and Outcomes</li>
                <ul>
                  <li>Readings: TBD</li>
                </ul>
              <li>Week 7: Quantifying uncertainty in preference models. Model-based estimates and the bootstrap.</li>
                <ul>
                  <li>Readings: TBD</li>
                </ul>
              <li>Week 8: Project</li>
              <li>Week 9: Project</li>
              <li>Week 10: Final Presentation</li>
            </ul>
            
            
            <h3>Nonlinear Regression (Winter 2020, Winter 2021, Spring 2021)</h3>
            <p class="text">
              <i> Students:</i> Oliver Bejar Tjalve, Alejandro Gonzalez, Muhammad Anas
            </p>
            <p class="text">
              <i> Prerequisites:</i> A basic knowledge of linear regression and some experience in R.
            </p>
            <p class="text">
              <i> Description:</i> Simple linear regression models can be easy to implement and interpret, but they don't always fit data well! 
              For this project, we'll explore regression methods that relax the assumption of linearity. These might include (based on the interest 
              and/or experience level of the student) polynomial regression, step functions, regression splines, smoothing splines, multivariate 
              adaptive regression splines, and generalized additive models. Hopefully, we'll even see how to validate such models using cross-validation! 
              We will mostly use James et al.'s "An Introduction to Statistical Learning" (ISLR) Chapter 7.
            </p>
            <p class="text">
              <i> Syllabus:</i>
            </p>
            <ul>
              <li>Week 1: Introductions</li>
              <li>Week 2: Review of Linear Regression</li>
                <ul>
                  <li>Readings: <a href="https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6062a083acbfe82c7195b27d/1617076404560/ISLR%2BSeventh%2BPrinting.pdf">Introduction to Statistical Learning in R</a> (ISLR) Chapters 3.1-3.3, Lab 2.3 and 3.6</li>
                </ul>
              <li>Week 3: Polynomial Regression, Step Functions, and Basis Functions </li>
                <ul>
                  <li>Readings: ISLR Chapters 7.1-7.3, Lab 7.8.1 </li>
                </ul>
              <li>Week 4: Regression Splines and Multivariate Adaptive Regression Splines (MARS)</li>
                <ul>
                  <li>Readings: ISLR Chapter 7.4; <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a> (ESL) Chapter 9.4</li>
                </ul>
              <li>Week 5: Smoothing Splines, Local Regression, and Kernel Regression</li>
                <ul>
                  <li>Readings: ISLR Chapter 7.5-7.6 and Lab 7.8.2; ESL Chapters 6.1-6.2</li>
                </ul>
              <li>Week 6: Generalized Additive Models (GAMs) and Project Preliminaries</li>
                <ul>
                  <li>Readings: ISLR Chapter 7.7, Lab 7.8.3</li>
                </ul>
              <li>Week 7: Project</li>
              <li>Week 8: Project</li>
              <li>Week 9: Project</li>
              <li>Week 10: Final Presentation</li>
            </ul>
            
            
            
            <h3>History and Practice of Data Communication (Autumn 2020)</h3>
            <p class="text">
              <i> Student:</i> Ziyi Li
            </p>
            <p class="text">
              <i> Prerequisites:</i> None; some experience with R or Python may be helpful but is not required.
            </p>
            <p class="text">
              <i> Description:</i> In this course, we'll learn about the development of data communication techniques and their modern use. We'll begin by 
              studying how people have visualized patterns in data over time, and consider how those methods reflected the computational resources 
              available in each era. Then, we'll shift our attention to modern issues in data communication, drawing examples from the COVID-19 
              pandemic and 2020 US presidential election: How do practitioners effectively show complex relationships or model uncertainty? How 
              do people mislead readers through text and figures (intentionally or otherwise)? What common pitfalls exist, and how can we avoid them? 
              We'll finish with a data communication project based on the student's interests.
            </p>
            <p class="text">
              <i> Syllabus:</i>
            </p>
            <ul>
              <li>Week 1: History of Data Visualization</li>
                <ul>
                  <li>Michael Friendly's “A Brief History of Data Visualization” (2006), chapters 1-2 (available online <a href="http://www.datavis.ca/papers/hbook.pdf">here</a>). </li>
                </ul>
              <li>Week 2: Methods I</li>
                <ul>
                  <li>Nathan Yau's "Visualize This: The FlowingData Guide to Design, Visualization, and Statistics" (2011). 
                  Chapters 1-4 (eBook available through 
                    <a href="https://alliance-primo.hosted.exlibrisgroup.com/permalink/f/kjtuig/CP51248438210001451">UW Libraries</a>). </li>
                </ul>
              <li>Week 3: Methods II</li>
                <ul>
                  <li>"Visualize This" Chapters 5, 6, and 9.</li>
                </ul>
              <li>Week 4: Data Journalism through the Election. This week, you'll read about two famous data journalism websites, as well as a few other examples and helpful tips for carryout data journalism. Focus on the big picture: What tools do these examples utilize, and what qualities seem to make data journalism effective?</li>
                    <ul>
                      <li>The Upshot (New York Times): <a href="https://www.nytimes.com/interactive/2019/04/22/upshot/upshot-at-five-years.html">Examples of work from 5-year retrospective</a>, <a href="https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html">pollster interpretation in 2016 election</a>, and <a href="https://www.nytimes.com/2019/02/28/reader-center/data-visualization-editor-amanda-cox.html">a brief history of the founder (and UW alumna!) Amanda Cox</a>.</li>
                      <li>FiveThirtyEight.com (ABC): <a href="https://projects.fivethirtyeight.com/2020-election-forecast/">2020 Election Forecasts</a>(be sure to check out presidential, house, and senate predictions), <a href="https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/">their methodology</a>, and <a href="https://nymag.com/news/features/51170/">some info on the founder, Nate Silver</a>. </li>
                      <li>Other Examples/Tips: <a href="https://parametric.press/issue-02/">Parametric Press</a> (new online journal, this issue on climate science), <a href="http://powerreporting.com/color/color_of_money.pdf">early example of data journalism on racial redlining in Atlanta</a> (won a Pulitzer Prize!). </li>
                    </ul>
              <li>Week 5: Visualizing Uncertainty through the Election. This week, you'll learn about the challenge of communicating uncertainty in results and predictions to the general public. 
                  Probability is a notoriously hard concept to truly understand!</li>
                   <ul>
                     <li>Nate Silver (of 538) <a href="https://fivethirtyeight.com/features/the-media-has-a-probability-problem/">article</a>.</li>
                     <li>The controversial NYT "Needle": <a href="https://www.nytimes.com/2017/12/14/reader-center/nyt-needle-election.html">Discussion</a>, <a href="https://www.youtube.com/watch?v=wz28wLEedFI">video of 2016 election night changes</a>, <a href="https://www.nytimes.com/2018/03/13/upshot/needle-forecast-pennsylvania-special-election.html">2018 version</a>, and <a href="https://www.nytimes.com/2020/02/03/upshot/needle-iowa-caucuses-faq.html">2020 version</a>. </li>
                     <li>Two potential solutions: <a href="https://medium.com/hci-design-at-uw/hypothetical-outcomes-plots-experiencing-the-uncertain-b9ea60d7c740">Hypothetical Outcome Plots</a>, and <a href="http://presidential-plinko.com/">Presidential Plinko</a>.</li>
                   </ul>
              <li>Week 6: Final Project: Preliminary Discussion</li>
                <ul>
                  <li><a href="https://www.theguardian.com/science/2014/mar/28/news-story-research-paper-wellcome-trust-science-writing-prize">How to write a science news article</a> (from The Guardian).</li>
                </ul>
              <li>Week 7: Misleading Readers Through Data. This week, we'll explore how content creators can mislead viewers using data, both intentionally and unintentionally.</li>
                <ul>
                   <li> <a href="https://cmci.colorado.edu/visualab/papers/p26-szafir.pdf">Danielle Szafir paper on ways visualizations can mislead</a>. </li>
                   <li> <a href="https://venngage.com/blog/misleading-graphs/">Infographic on misleading graphs</a>. </li>
                   <li> Quiz from the UW Center for an Informed Public: <a href="https://www.spotdeepfakes.org/en-US">Spot the Deepfake</a>. </li>
                </ul>
              <li>Week 8: Visualizing COVID. This week we'll critique data visualizations and communications on a timely subject: COVID-19. For each of the readings below, write down what you believe are strengths and weaknesses of the visualizations and writing (when applicable). What factors and other motivations may be influencing the author/creator when publishing this content? </li>
                <ul>
                  <li> <a href="https://www.nytimes.com/interactive/2020/11/18/us/covid-state-restrictions.html">Charts exploring the worsening outbreaks in each state by level of restrictions</a> (NYT). </li>
                  <li> <a href="https://covid19risk.biosci.gatech.edu/">COVID event risk assessment tool</a>. </li>
                  <li> <a href="https://www.washington.edu/coronavirus/testing-results/?mkt_tok=eyJpIjoiWmpCa05UWmtZekJpTWpsbSIsInQiOiJpZG1adEpzNUhoTVVBSTlPTzcyN3VEZTdQMzhDazB4K2RhNzhiaUF0U3UzbWF2bDdNb3hMdTNrd1ZCRVwvNmV6S1kzYUl5TDJoKzBJamp3UEdEcVdBZWIxYXpQVjZGXC9WbFoxdTl4UUR5YnJ2Tk5zUkprQVpaR1kwQ2ExamdtOFF0In0%3D">UW's COVID case tracking "dashboard"</a>. </li>
                  <li> <a href="https://coronaboard.com/global/">Worldwide COVID dashboard</a> (published by an independent creator). </li>
                  <li> <a href="https://qz.com/1872980/how-bad-covid-19-data-visualizations-mislead-the-public/">A critique of US state-level dashboards</a> (Quartz; somewhat outdated but still helpful). </li>
                </ul>
              <li>Week 9: Practice Presentation</li>
              <li>Week 10: Project Practice and Final Presentation </li>
            </ul>
            
      </div>
    </div>
  </div>
</body>

</html>
